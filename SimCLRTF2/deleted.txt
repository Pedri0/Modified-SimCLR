Deleted options/funcs in original simclrtf2 

version 16 Enero 2021: last commit 18 Diciembre 2020:

Run.py:

    Flags :

        -- flags.DEFINE_bool(
        'cache_dataset', False,
        'Whether to cache the entire dataset in memory. If the dataset is '
        'ImageNet, this is a very bad idea, but for smaller datasets it can '
        'improve performance.')

        -- flags.DEFINE_string(
        'master', None,
        'Address/name of the TensorFlow master to use. By default, use an '
        'in-process master.')

        --flags.DEFINE_bool(
        'use_tpu', True,
        'Whether to run on TPU.')

        --flags.DEFINE_string(
        'tpu_name', None,
        'The Cloud TPU to use for training. This should be either the name '
        'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 '
        'url.')

        --flags.DEFINE_string(
        'tpu_zone', None,
        '[Optional] GCE zone where the Cloud TPU is located in. If not '
        'specified, we will attempt to automatically detect the GCE project from '
        'metadata.')

        --flags.DEFINE_string(
        'gcp_project', None,
        '[Optional] Project name for the Cloud TPU-enabled project. If not '
        'specified, we will attempt to automatically detect the GCE project from '
        'metadata.')


    Lines 488 - 501 from original run.py script:
        if FLAGS.use_tpu:
            if FLAGS.tpu_name:
            cluster = tf.distribute.cluster_resolver.TPUClusterResolver(
                FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)
            else:
            cluster = tf.distribute.cluster_resolver.TPUClusterResolver(FLAGS.master)
            tf.config.experimental_connect_to_cluster(cluster)
            topology = tf.tpu.experimental.initialize_tpu_system(cluster)
            logging.info('Topology:')
            logging.info('num_tasks: %d', topology.num_tasks)
            logging.info('num_tpus_per_task: %d', topology.num_tpus_per_task)
            strategy = tf.distribute.experimental.TPUStrategy(cluster)

resnet.py:
    def resnet Lines 702-747:
        101: {
          'block': BottleneckBlock,
          'layers': [3, 4, 23, 3]
        },
        152: {
            'block': BottleneckBlock,
            'layers': [3, 8, 36, 3]
        },
        200: {
            'block': BottleneckBlock,
            'layers': [3, 24, 36, 3]
        }
        }

    def class ResidualBlock() Lines 314-382:
        --330 - 345
            if FLAGS.sk_ratio > 0:  # Use ResNet-D (https://arxiv.org/abs/1812.01187)
                if strides > 1:
                self.shortcut_layers.append(FixedPadding(2, data_format))
                self.shortcut_layers.append(
                    tf.keras.layers.AveragePooling2D(
                        pool_size=2,
                        strides=strides,
                        padding='SAME' if strides == 1 else 'VALID',
                        data_format=data_format))
                self.shortcut_layers.append(
                    Conv2dFixedPadding(
                        filters=filters,
                        kernel_size=1,
                        strides=1,
                        data_format=data_format))
            else:

        -- 367 - 368
            if FLAGS.se_ratio > 0:
                self.se_layer = SE_Layer(filters, FLAGS.se_ratio, data_format=data_format)
        
        -- 379 - 380
            if FLAGS.se_ratio > 0:
                inputs = self.se_layer(inputs, training=training)

    def class BottleneckBlock() Lines 385-487
        -- 400 - 414
            if FLAGS.sk_ratio > 0:  # Use ResNet-D (https://arxiv.org/abs/1812.01187)
                if strides > 1:
                self.projection_layers.append(FixedPadding(2, data_format))
                self.projection_layers.append(
                    tf.keras.layers.AveragePooling2D(
                        pool_size=2,
                        strides=strides,
                        padding='SAME' if strides == 1 else 'VALID',
                        data_format=data_format))
                self.projection_layers.append(
                    Conv2dFixedPadding(
                        filters=filters_out,
                        kernel_size=1,
                        strides=1,
                        data_format=data_format))
        
        -- 442 - 445
            if FLAGS.sk_ratio > 0:
                self.conv_relu_dropblock_layers.append(
                    SK_Conv2D(filters, strides, FLAGS.sk_ratio, data_format=data_format))
            else:

        -- 474-476
            if FLAGS.se_ratio > 0:
                self.conv_relu_dropblock_layers.append(
            SE_Layer(filters, FLAGS.se_ratio, data_format=data_format))
    
    def class DropBlock() Lines 81 - 157

        -- 100 - 102
            tf.logging.info(
                'Applying DropBlock: dropblock_size {}, net.shape {}'.format(
                 dropblock_size, net.shape))

    def class Resnet () Lines 529 - 699

        -- 566 - 592
            if FLAGS.sk_ratio > 0:  # Use ResNet-D (https://arxiv.org/abs/1812.01187)
            self.initial_conv_relu_max_pool.append(
                Conv2dFixedPadding(
                    filters=64 * width_multiplier // 2,
                    kernel_size=3,
                    strides=2,
                    data_format=data_format,
                    trainable=trainable))
            self.initial_conv_relu_max_pool.append(
                BatchNormRelu(data_format=data_format, trainable=trainable))
            self.initial_conv_relu_max_pool.append(
                Conv2dFixedPadding(
                    filters=64 * width_multiplier // 2,
                    kernel_size=3,
                    strides=1,
                    data_format=data_format,
                    trainable=trainable))
            self.initial_conv_relu_max_pool.append(
                BatchNormRelu(data_format=data_format, trainable=trainable))
            self.initial_conv_relu_max_pool.append(
                Conv2dFixedPadding(
                    filters=64 * width_multiplier,
                    kernel_size=3,
                    strides=1,
                    data_format=data_format,
                    trainable=trainable))
            else:

        -- 679 - 681:
            if FLAGS.train_mode == 'finetune' and FLAGS.fine_tune_after_block == 4:
            # This case doesn't really matter.
            trainable = True


model.py

    class Model Lines 229-280:
        -- 272 - 278:
            elif FLAGS.train_mode == 'pretrain' and FLAGS.lineareval_while_pretraining:
        # When performing pretraining and linear evaluation together we do not
        # want information from linear eval flowing back into pretraining network
        # so we put a stop_gradient.
        supervised_head_outputs = self.supervised_head(
            tf.stop_gradient(supervised_head_inputs), training)
        return projection_head_outputs, supervised_head_outputs   




