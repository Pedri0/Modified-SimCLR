Deleted options/funcs in original simclrtf2 

version 16 Enero 2021: last commit 18 Diciembre 2020:

Flags :

    -- flags.DEFINE_bool(
    'cache_dataset', False,
    'Whether to cache the entire dataset in memory. If the dataset is '
    'ImageNet, this is a very bad idea, but for smaller datasets it can '
    'improve performance.')

    -- flags.DEFINE_string(
    'master', None,
    'Address/name of the TensorFlow master to use. By default, use an '
    'in-process master.')

    --flags.DEFINE_bool(
    'use_tpu', True,
    'Whether to run on TPU.')

    --flags.DEFINE_string(
    'tpu_name', None,
    'The Cloud TPU to use for training. This should be either the name '
    'used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 '
    'url.')

    --flags.DEFINE_string(
    'tpu_zone', None,
    '[Optional] GCE zone where the Cloud TPU is located in. If not '
    'specified, we will attempt to automatically detect the GCE project from '
    'metadata.')

    --flags.DEFINE_string(
    'gcp_project', None,
    '[Optional] Project name for the Cloud TPU-enabled project. If not '
    'specified, we will attempt to automatically detect the GCE project from '
    'metadata.')


Lines 488 - 501 from original run.py script:
    if FLAGS.use_tpu:
        if FLAGS.tpu_name:
        cluster = tf.distribute.cluster_resolver.TPUClusterResolver(
            FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)
        else:
        cluster = tf.distribute.cluster_resolver.TPUClusterResolver(FLAGS.master)
        tf.config.experimental_connect_to_cluster(cluster)
        topology = tf.tpu.experimental.initialize_tpu_system(cluster)
        logging.info('Topology:')
        logging.info('num_tasks: %d', topology.num_tasks)
        logging.info('num_tpus_per_task: %d', topology.num_tpus_per_task)
        strategy = tf.distribute.experimental.TPUStrategy(cluster)


